{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2026 Forward Test — Out-of-Sample Evaluation\n",
    "\n",
    "Uses the **last trained models** (L1+L2 window 31, RL agent window 31) from the\n",
    "2018-2025 backtest to trade on unseen 2026 data (Jan–Feb 2026).\n",
    "\n",
    "**No retraining** — this is a pure out-of-sample test.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed 2018-2025 backtest (`run_id=20260228_144143`) with models on Drive\n",
    "- 2026 OHLCV data on Drive (or will be fetched via baostock)\n",
    "\n",
    "**Runtime:** T4 GPU recommended (XGBoost inference is faster on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup — mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/kronos'\n",
    "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "print(f'Drive root: {DRIVE_ROOT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Clone/update repo + install deps\n",
    "import sys\n",
    "\n",
    "REPO_DIR = '/content/tradingagent'\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/Yuxiaoliu12/tradingagent.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull origin master\n",
    "\n",
    "!pip install -q -r {REPO_DIR}/requirements.txt\n",
    "\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "print(f'Repo at: {REPO_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Ensure 2026 OHLCV data exists on Drive\n",
    "# If the existing pickle doesn't cover 2026, fetch fresh data via baostock.\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "OHLCV_PATH = os.path.join(DRIVE_ROOT, 'data/ohlcv_all_a.pkl')\n",
    "BENCH_PATH = os.path.join(DRIVE_ROOT, 'data/benchmark_000905.pkl')\n",
    "\n",
    "# Check if 2026 data already exists\n",
    "need_update = True\n",
    "if os.path.exists(OHLCV_PATH):\n",
    "    with open(OHLCV_PATH, 'rb') as f:\n",
    "        ohlcv_check = pickle.load(f)\n",
    "    sample_sym = next(iter(ohlcv_check))\n",
    "    latest = ohlcv_check[sample_sym].index.max()\n",
    "    print(f'Existing OHLCV: {len(ohlcv_check)} symbols, latest date: {latest.date()}')\n",
    "    if latest >= pd.Timestamp('2026-01-15'):\n",
    "        need_update = False\n",
    "        print('2026 data already present — skipping download.')\n",
    "    else:\n",
    "        print('No 2026 data found — will fetch via baostock.')\n",
    "    del ohlcv_check\n",
    "else:\n",
    "    print(f'No OHLCV file at {OHLCV_PATH} — will fetch via baostock.')\n",
    "\n",
    "if need_update:\n",
    "    !pip install -q baostock\n",
    "    import baostock as bs\n",
    "    lg = bs.login()\n",
    "    print(f'baostock login: {lg.error_msg}')\n",
    "\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "    from screener.download_ohlcv import (\n",
    "        get_all_a_shares, download_universe_ohlcv,\n",
    "        download_benchmark, download_industry_mapping,\n",
    "    )\n",
    "\n",
    "    symbols = get_all_a_shares()\n",
    "    # Filter out index codes (sz.399xxx bug)\n",
    "    symbols = [s for s in symbols if not s.startswith('sz.399')]\n",
    "    print(f'After filtering indices: {len(symbols)} stocks')\n",
    "\n",
    "    download_universe_ohlcv(\n",
    "        symbols, start_date='2015-01-01', end_date='2026-03-01',\n",
    "        save_path=OHLCV_PATH,\n",
    "    )\n",
    "    download_benchmark(\n",
    "        start_date='2015-01-01', end_date='2026-03-01',\n",
    "        save_path=BENCH_PATH,\n",
    "    )\n",
    "    download_industry_mapping(\n",
    "        save_path=os.path.join(DRIVE_ROOT, 'data/industry_mapping.pkl'),\n",
    "    )\n",
    "    bs.logout()\n",
    "    print('Data download complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Config + load data\nfrom screener.config import ScreenerConfig\n\ncfg = ScreenerConfig(\n    ohlcv_pickle_path=os.path.join(DRIVE_ROOT, 'data/ohlcv_all_a.pkl'),\n    benchmark_pickle_path=os.path.join(DRIVE_ROOT, 'data/benchmark_000905.pkl'),\n    industry_pickle_path=os.path.join(DRIVE_ROOT, 'data/industry_mapping.pkl'),\n    drive_root=os.path.join(DRIVE_ROOT, 'output/screener'),\n    # Keep pinned run_id to find trained models\n    run_id='20260228_144143',\n    # Extend backtest_end so load_data() includes 2026 in calendar + OHLCV\n    backtest_end='2026-03-31',\n)\n\n# GPU XGBoost (Colab T4)\ncfg.layer1_xgb_params['device'] = 'cuda'\ncfg.layer2_xgb_params['device'] = 'cuda'\n\nprint(f'Run ID: {cfg.run_id}')\nprint(f'Run dir: {cfg.run_dir}')\nprint(f'Cache dir: {cfg.cache_dir}')\nprint(f'Backtest end: {cfg.backtest_end}')\n\n# Load data (OHLCV + Alpha158 + regime + calendar)\nfrom screener.backtester import WalkForwardBacktester\n\nbt = WalkForwardBacktester(cfg)\nbt.load_data()\n\n# Verify 2026 coverage\ncal_2026 = bt._calendar[bt._calendar >= '2026-01-01']\nprint(f'\\n2026 trading days in calendar: {len(cal_2026)}')\nif len(cal_2026) > 0:\n    print(f'  Range: {cal_2026[0].date()} → {cal_2026[-1].date()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load last trained L1+L2 models + generate 2026 signals\n",
    "import time\n",
    "\n",
    "# The 2018-2025 backtest produced 32 quarterly windows (0-31).\n",
    "# Window 31 = Q4 2025 — the most recent trained models.\n",
    "LAST_WINDOW = 31\n",
    "\n",
    "# Verify model files exist\n",
    "l1l2_path = os.path.join(cfg.cache_dir, 'l1l2_models', f'window_{LAST_WINDOW}.pkl')\n",
    "rl_model_path = os.path.join(cfg.run_dir, f'rl_model_window_{LAST_WINDOW}')\n",
    "\n",
    "print(f'L1+L2 model path: {l1l2_path}')\n",
    "print(f'  Exists: {os.path.exists(l1l2_path)}')\n",
    "print(f'RL model path: {rl_model_path}.zip')\n",
    "print(f'  Exists: {os.path.exists(rl_model_path + \".zip\")}')\n",
    "\n",
    "if not os.path.exists(l1l2_path):\n",
    "    raise FileNotFoundError(\n",
    "        f'No L1+L2 models at {l1l2_path}. '\n",
    "        f'Run the full 2018-2025 backtest first (train_colab.ipynb).'\n",
    "    )\n",
    "if not os.path.exists(rl_model_path + '.zip'):\n",
    "    raise FileNotFoundError(\n",
    "        f'No RL model at {rl_model_path}. '\n",
    "        f'Run the full 2018-2025 backtest first (train_colab.ipynb).'\n",
    "    )\n",
    "\n",
    "# Load L1+L2 models into the backtester\n",
    "bt._load_window_models(LAST_WINDOW)\n",
    "\n",
    "# Ensure lagged IC covers 2026\n",
    "bt.layer1.ensure_lagged_ic(range(2025, 2027))\n",
    "\n",
    "# Generate L1+L2 signals for 2026\n",
    "TEST_START = '2026-01-01'\n",
    "TEST_END = '2026-02-28'  # adjust as data becomes available\n",
    "\n",
    "t0 = time.time()\n",
    "print(f'\\nGenerating signals for {TEST_START} → {TEST_END}...')\n",
    "signals_2026 = bt._generate_daily_signals(TEST_START, TEST_END, verbose=True)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f'\\nGenerated {len(signals_2026)} daily signals in {elapsed:.0f}s')\n",
    "\n",
    "# Quick signal quality check\n",
    "non_empty = sum(1 for s in signals_2026 if len(s['l2_ranking']) > 0)\n",
    "print(f'Days with L2 candidates: {non_empty}/{len(signals_2026)}')\n",
    "if signals_2026:\n",
    "    avg_candidates = sum(len(s['l2_ranking']) for s in signals_2026) / len(signals_2026)\n",
    "    print(f'Avg candidates per day: {avg_candidates:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: RL inference on 2026\n",
    "import numpy as np\n",
    "from screener.portfolio_env import PortfolioEnv\n",
    "from screener.rl_trader import RLTrader\n",
    "from screener.data_pipeline import _get_benchmark_cache\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "\n",
    "benchmark_df = _get_benchmark_cache(cfg)\n",
    "\n",
    "# Load the last trained RL model\n",
    "rl_trader = RLTrader(cfg)\n",
    "model = rl_trader.load(rl_model_path)\n",
    "print(f'RL model loaded from {rl_model_path}')\n",
    "\n",
    "# Create test environment\n",
    "test_env = PortfolioEnv(\n",
    "    cfg, signals_2026, bt._ohlcv,\n",
    "    benchmark_df, training_mode=False,\n",
    "    candidate_mode='top',\n",
    ")\n",
    "masked_env = ActionMasker(test_env, lambda e: e.action_masks())\n",
    "\n",
    "# Run inference\n",
    "obs, _ = masked_env.reset()\n",
    "total_reward = 0.0\n",
    "blocked_count = 0\n",
    "sub_count = 0\n",
    "\n",
    "print(f'\\nRunning RL inference on {len(signals_2026)} trading days...')\n",
    "for step_i in range(len(signals_2026) - 1):\n",
    "    masks = get_action_masks(masked_env)\n",
    "    action, _ = model.predict(obs, deterministic=True, action_masks=masks)\n",
    "    obs, reward, terminated, truncated, info = masked_env.step(int(action))\n",
    "    total_reward += reward\n",
    "    blocked_count += len(info.get('blocked_trades', []))\n",
    "    sub_count += len(info.get('substituted_trades', []))\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f'\\nInference complete: {step_i+1} steps')\n",
    "print(f'  Total reward: {total_reward:.2f}')\n",
    "print(f'  Blocked trades: {blocked_count}')\n",
    "print(f'  Substituted trades: {sub_count}')\n",
    "\n",
    "# Extract results\n",
    "nav_history = test_env._nav_history\n",
    "trade_log = test_env.trade_log\n",
    "dates_2026 = [s['date'] for s in signals_2026]\n",
    "\n",
    "# Build NAV series\n",
    "nav_series = pd.Series(\n",
    "    {dates_2026[i]: nav_history[i] for i in range(min(len(dates_2026), len(nav_history)))},\n",
    "    name='nav',\n",
    ").sort_index()\n",
    "\n",
    "# Compute metrics\n",
    "total_return = nav_series.iloc[-1] / nav_series.iloc[0] - 1\n",
    "daily_rets = nav_series.pct_change().dropna()\n",
    "sharpe = (daily_rets.mean() / daily_rets.std() * np.sqrt(252)\n",
    "          if daily_rets.std() > 0 else 0.0)\n",
    "drawdown = (nav_series / nav_series.cummax() - 1)\n",
    "max_dd = drawdown.min()\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'2026 FORWARD TEST RESULTS')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'  Period:       {nav_series.index[0].date()} → {nav_series.index[-1].date()}')\n",
    "print(f'  Trading days: {len(nav_series)}')\n",
    "print(f'  Total return: {total_return*100:+.2f}%')\n",
    "print(f'  Sharpe ratio: {sharpe:.2f}')\n",
    "print(f'  Max drawdown: {max_dd*100:.2f}%')\n",
    "print(f'  Total trades: {len(trade_log)}')\n",
    "print(f'  Final NAV:    {nav_series.iloc[-1]:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualise results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), gridspec_kw={'height_ratios': [3, 1, 1]})\n",
    "\n",
    "# --- Panel 1: NAV curve vs benchmark ---\n",
    "ax = axes[0]\n",
    "norm_nav = nav_series / nav_series.iloc[0]\n",
    "norm_nav.plot(ax=ax, label='RL Agent', linewidth=2, color='#2196F3')\n",
    "\n",
    "# Overlay benchmark (CSI 500)\n",
    "bench_2026 = benchmark_df.loc[\n",
    "    (benchmark_df.index >= nav_series.index[0]) &\n",
    "    (benchmark_df.index <= nav_series.index[-1]),\n",
    "    'close'\n",
    "]\n",
    "if len(bench_2026) > 0:\n",
    "    norm_bench = bench_2026 / bench_2026.iloc[0]\n",
    "    norm_bench.plot(ax=ax, label='CSI 500', linewidth=1.5, color='#FF9800', alpha=0.7)\n",
    "\n",
    "ax.set_title('2026 Forward Test: RL Agent vs CSI 500', fontsize=14)\n",
    "ax.set_ylabel('Growth of $1')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# --- Panel 2: Daily returns ---\n",
    "ax = axes[1]\n",
    "colors = ['#4CAF50' if r >= 0 else '#F44336' for r in daily_rets]\n",
    "ax.bar(daily_rets.index, daily_rets.values * 100, color=colors, width=1.5)\n",
    "ax.set_ylabel('Daily Return (%)')\n",
    "ax.set_title('Daily Returns', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Panel 3: Drawdown ---\n",
    "ax = axes[2]\n",
    "drawdown_pct = drawdown * 100\n",
    "ax.fill_between(drawdown_pct.index, drawdown_pct.values, 0, color='#F44336', alpha=0.3)\n",
    "ax.plot(drawdown_pct.index, drawdown_pct.values, color='#F44336', linewidth=1)\n",
    "ax.set_ylabel('Drawdown (%)')\n",
    "ax.set_title('Drawdown', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Trade summary ---\n",
    "if trade_log:\n",
    "    trades_df = pd.DataFrame(trade_log)\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print('TRADE SUMMARY')\n",
    "    print(f'{\"=\"*60}')\n",
    "    buys = trades_df[trades_df['action'] == 'buy']\n",
    "    sells = trades_df[trades_df['action'] == 'sell']\n",
    "    print(f'  Buy trades:  {len(buys)}')\n",
    "    print(f'  Sell trades: {len(sells)}')\n",
    "    if len(sells) > 0 and 'pnl' in sells.columns:\n",
    "        winners = (sells['pnl'] > 0).sum()\n",
    "        losers = (sells['pnl'] <= 0).sum()\n",
    "        print(f'  Win rate:    {winners/(winners+losers)*100:.1f}%')\n",
    "        print(f'  Avg P&L:     {sells[\"pnl\"].mean():+.2f}')\n",
    "        print(f'  Avg hold:    {sells[\"hold_days\"].mean():.1f} days')\n",
    "    print(f'\\n  Unique stocks traded: {trades_df[\"symbol\"].nunique()}')\n",
    "    print(f'  Most traded: {trades_df[\"symbol\"].value_counts().head(5).to_string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Position history — daily holdings with stock names\n",
    "\n",
    "# Reconstruct daily positions from trade log\n",
    "positions = {}  # date -> {symbol: {shares, entry_price, hold_days}}\n",
    "current_holdings = {}\n",
    "\n",
    "if trade_log:\n",
    "    trades_df = pd.DataFrame(trade_log)\n",
    "    for date in sorted(trades_df['date'].unique()):\n",
    "        day_trades = trades_df[trades_df['date'] == date]\n",
    "        for _, t in day_trades.iterrows():\n",
    "            sym = t['symbol']\n",
    "            if t['action'] == 'buy':\n",
    "                current_holdings[sym] = {\n",
    "                    'shares': t['shares'],\n",
    "                    'price': t['price'],\n",
    "                }\n",
    "            elif t['action'] == 'sell':\n",
    "                current_holdings.pop(sym, None)\n",
    "        positions[date] = dict(current_holdings)\n",
    "\n",
    "# Look up stock names via baostock\n",
    "all_syms = set()\n",
    "for pos in positions.values():\n",
    "    all_syms.update(pos.keys())\n",
    "\n",
    "sym_names = {}\n",
    "if all_syms:\n",
    "    try:\n",
    "        import baostock as bs\n",
    "        lg = bs.login()\n",
    "        for sym in sorted(all_syms):\n",
    "            rs = bs.query_stock_basic(code=sym)\n",
    "            row = rs.get_data()\n",
    "            if not row.empty:\n",
    "                name = row.iloc[0].get('code_name', sym)\n",
    "                sym_names[sym] = name\n",
    "            else:\n",
    "                sym_names[sym] = sym\n",
    "        bs.logout()\n",
    "    except Exception as e:\n",
    "        print(f'Could not fetch stock names: {e}')\n",
    "        sym_names = {s: s for s in all_syms}\n",
    "\n",
    "# Display daily holdings\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('DAILY POSITION HISTORY')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "for date in sorted(positions.keys()):\n",
    "    pos = positions[date]\n",
    "    if pos:\n",
    "        holdings_str = ', '.join(\n",
    "            f'{sym_names.get(s, s)} ({s}) x{p[\"shares\"]:.0f}'\n",
    "            for s, p in pos.items()\n",
    "        )\n",
    "    else:\n",
    "        holdings_str = '(cash)'\n",
    "    print(f'{date.date() if hasattr(date, \"date\") else date}: {holdings_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save results to Drive\n",
    "import json\n",
    "\n",
    "results_dir = os.path.join(cfg.run_dir, 'forward_test_2026')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save full results\n",
    "results = {\n",
    "    'nav_series': nav_series,\n",
    "    'trade_log': trade_log,\n",
    "    'metrics': {\n",
    "        'total_return': float(total_return),\n",
    "        'sharpe': float(sharpe),\n",
    "        'max_drawdown': float(max_dd),\n",
    "        'n_days': len(nav_series),\n",
    "        'n_trades': len(trade_log),\n",
    "        'test_start': TEST_START,\n",
    "        'test_end': TEST_END,\n",
    "        'model_window': LAST_WINDOW,\n",
    "    },\n",
    "}\n",
    "\n",
    "pkl_path = os.path.join(results_dir, 'forward_test_results.pkl')\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# Also save a human-readable JSON summary\n",
    "json_path = os.path.join(results_dir, 'forward_test_summary.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results['metrics'], f, indent=2)\n",
    "\n",
    "print(f'Results saved to:')\n",
    "print(f'  {pkl_path}')\n",
    "print(f'  {json_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}